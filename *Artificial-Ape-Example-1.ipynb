{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f3f879",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "This code corresponds to experiment (iii.1) and (iii.2) in the paper. To change the experiment, modify the dataset EXP variable (see cell below)\n",
    "\n",
    "## Environment\n",
    "\n",
    "In this environment the agent is placed atop a platform, it can look left or right, or remain facing in the current direction. There are some cubes that move up/down (perpendicular to the view changes). In this example the goal is to disentangle these moving cubes (exafference) from the changes in view caused by the agents action (reafference).\n",
    "\n",
    "Instead of using the environment directly, we use a dataset that has been generated using the world-of-bugs platform. World of Bugs is a platform that is otherwise used for automated bug detection, but has a nice collection of environments built in the Unity3D engine. We have adapted one of these environments to produce the artificial ape environment.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Action Space\n",
    "The action is a `ndarray` with shape `(1,)` which can take values `{0, 1, 2}` indicating the direction of the view rotation. The view is rotated by a small amount (2 degrees).\n",
    "\n",
    "| Num | Action                 |\n",
    "|-----|------------------------|\n",
    "| 0   | Look Reft              | \n",
    "| 1   | Noop                   |\n",
    "| 2   | Look Right             |\n",
    "\n",
    "\n",
    "### Observation Space\n",
    "\n",
    "The full observation has been cropped to an 64 x 64 image. The observation are in a pytorch compatible format (float, CHW, [0-1]).\n",
    "\n",
    "The observation is a `ndarray` with shape `(1,64,64)` an image.\n",
    "\n",
    "### More info\n",
    "Install this repo as a dependency:\n",
    "\n",
    "```\n",
    "import sys\n",
    "!{sys.executable} -m pip install ./reafference\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a89b2334",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import gym\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import reafference.jnu as J\n",
    "from reafference.environment.artificialape import make_dataset, load\n",
    "from reafference.model import UNet, DiagLinear\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "# CHANGE THIS PATH TO THE DATASET LOCATION IF NEEDED!\n",
    "# experiment (iii.1), experiment (iii.2), other \n",
    "EXP = 1 # experiment 1 or 2\n",
    "DATASET = ['no-platform', 'platformi', 'platform']\n",
    "PATH = f\"./data/artificial-ape/{DATASET[EXP-1]}/train/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83c1c4d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ded241f5ed646b7a0cf96780c758b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='x', layout=Layout(width='99%'), max=1022), Output()), _d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315b9e5d4beb43fd957202a054744fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Canvas(height=201, width=384),), layout=Layout(align_items='center', display='flex', flex_flow=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<reafference.jnu.image._image.Image at 0x7f0cfe5c0b80>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Exploration\n",
    "dataset = load(PATH + \"episode-0.tar\")\n",
    "state, action, rotation = [np.stack(z) for z in zip(*dataset)]\n",
    "gt_effect = ((state[1:] - state[:-1]) + 1) / 2\n",
    "imgs = np.concatenate([state[:-1],gt_effect], axis=3)\n",
    "aimg = resize(torch.from_numpy(np.eye(3)[action[:-1]]).unsqueeze(1).unsqueeze(1), size=(3, imgs.shape[-1]), interpolation=0).numpy()\n",
    "imgs = np.concatenate([aimg, imgs], axis=-2)\n",
    "# unfortunately we cannot visualise the ground-truth reafferent/exafferent effect as its not avaliable in the dataset.\n",
    "J.images(imgs, scale=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75b3872e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96ff64eb7084abbba6b065c5bd4e1ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make dataset for training\n",
    "dataset = make_dataset(*glob.glob(PATH + \"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9269d1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNet to estimate effects\n",
    "state_shape = [1,64,64]\n",
    "action_shape = [3]\n",
    "latent_shape = [512]\n",
    "epochs = 50\n",
    "\n",
    "model = UNet(state_shape[0], state_shape[0], exp=4, output_activation=torch.nn.Tanh(), batch_normalize=False)\n",
    "conditional_shape = model.conditional_shape(state_shape)\n",
    "model.condition = DiagLinear(conditional_shape, action_shape=action_shape[0])\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "import torchinfo\n",
    "torchinfo.summary(model, input_data=(torch.zeros(2, *state_shape), torch.zeros(2, *action_shape)), device=DEVICE)\n",
    "\n",
    "# pretrained models\n",
    "# experiment (iii.1) Artificial-Ape-1.model.pt\n",
    "# experiment (iii.2) Artificial-Ape-2.model.pt\n",
    "# experiment (iii.2 no indicator) Artificial-Ape-3.model.pt\n",
    "model.load_state_dict(torch.load(f\"./models/Artificial-Ape-{EXP}.model.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a89eef1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06b4558f93e453395ee2d69804d819d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=False)\n",
    "    \n",
    "# Train\n",
    "epoch_iter = tqdm(range(epochs))\n",
    "\n",
    "for e in epoch_iter:\n",
    "    avg_loss = []\n",
    "    for x1, x2, a in loader:\n",
    "        #print(x[...,2].min(), x[...,2].max())\n",
    "        x1, x2, a = x1.to(DEVICE), x2.to(DEVICE), a.to(DEVICE)\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # prediction of the total effect of each action\n",
    "        pred_total_effect = model(x1, a)        \n",
    "        # prediction of the exafferent effect - when all actions are noop  \n",
    "        noop = torch.zeros_like(a)\n",
    "        noop[:,1] = 1. # noop is at index 1!\n",
    "        pred_exafferent_effect = model(x1, noop)\n",
    "        \n",
    "        # prediction of the reafferent effect (total - exafferent)\n",
    "        # this will be 0 for any a == 0 in the batch\n",
    "        pred_reafferent_effect = pred_total_effect - pred_exafferent_effect.detach()\n",
    "        #pred_reafferent_effect[a[:,0] == 1] = 0. # detach gradients where reafferent effect should be 0 (?)\n",
    "    \n",
    "        pred_effect = pred_exafferent_effect + pred_reafferent_effect # combined effect\n",
    "        total_effect = x2 - x1 # ground truth total effect\n",
    "        \n",
    "        loss = criterion(pred_effect, total_effect)\n",
    "        loss.backward()\n",
    "        avg_loss.append(loss.detach())\n",
    "        optim.step()\n",
    "    \n",
    "    avg_loss = torch.stack(avg_loss).cpu().numpy().mean()\n",
    "    epoch_iter.set_description(f\"Loss: {avg_loss : .5f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb9ba936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764adb65560a4d17bbca9bac0662f5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='x', layout=Layout(width='99%'), max=1022), Output()), _d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbbdb0d35bd4659b2a9d460a02763aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Canvas(height=201, width=975),), layout=Layout(align_items='center', display='flex', flex_flow=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision.transforms.functional as fn\n",
    "\n",
    "def get_images(state, action, total_effect, t_effect, re_effect, ex_effect):\n",
    "    b = -torch.ones_like(total_effect[...,:1]).cpu()\n",
    "    imgs = (torch.cat([b, total_effect.cpu(), b, t_effect.cpu(), b, re_effect.cpu(), b, ex_effect.cpu(), b], dim=-1) + 1) / 2\n",
    "    imgs = torch.cat([state[:-1].cpu(), imgs], dim=-1)\n",
    "    ai = fn.resize(torch.eye(3)[action[:-1]].unsqueeze(1).unsqueeze(1), size=(3,imgs.shape[-1]), interpolation=0)\n",
    "    imgs = torch.cat([imgs, ai], dim=-2)\n",
    "    return imgs\n",
    "\n",
    "state, action, rotation = [np.stack(z) for z in zip(*load(PATH.replace(\"train\", \"test\") + \"episode-0.tar\"))]\n",
    "\n",
    "with torch.no_grad():\n",
    "    x1, a = torch.from_numpy(state[:-1]).to(DEVICE), torch.eye(3)[torch.from_numpy(action[:-1])].to(DEVICE)\n",
    "    pred_total = model(x1, a)\n",
    "    noop = torch.zeros_like(a)\n",
    "    noop[:,1] = 1. \n",
    "    pred_ex = model(x1, noop)\n",
    "    pred_re = pred_total - pred_ex\n",
    "\n",
    "gt_effect = torch.from_numpy((state[1:] - state[:-1]))\n",
    "imgs = get_images(torch.from_numpy(state), torch.from_numpy(action), gt_effect, pred_total, pred_re, pred_ex)\n",
    "    \n",
    "J.images(imgs, scale=3, on_interact=action)\n",
    "\n",
    "imgs = fn.resize(imgs, size=[2*imgs.shape[2], 2*imgs.shape[3]], interpolation=torchvision.transforms.functional.InterpolationMode.NEAREST)\n",
    "\n",
    "video = (imgs.permute(0,2,3,1) * 255).int().repeat(1,1,1,3)[:200]\n",
    "torchvision.io.write_video(f\"./media/ArtificialApe-{EXP}-Predictions.mp4\", video, fps=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aec00c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./models/Artificial-Ape-3.model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf26cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reaff",
   "language": "python",
   "name": "reaff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
