{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f3f879",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "Your objective is to guide your chicken across lane after lane of busy rush hour traffic. You receive a point for every chicken that makes it to the top of the screen after crossing all the lanes of traffic. We use the `FreewayDeterminstic-v4` as it has a restricted actions space (that contains the required noop action), and is determinstic and so it is easier to visualise/interpret performance.\n",
    "\n",
    "\n",
    "### Modifications to FreewayDeterministic-v4\n",
    "\n",
    "The full observation has been cropped to an 84 x 84 image. This reduces the computational requirments but still illustrates the point. The observations also been converted to a pytorch compatible format (float, CHW, [0-1]).\n",
    "\n",
    "\n",
    "### Action Space\n",
    "The action is a `ndarray` with shape `(1,)` which can take values `{0, 1, 2}` indicating the direction of movement.\n",
    "\n",
    "| Num | Action                 |\n",
    "|-----|------------------------|\n",
    "| 0   | Noop                   | \n",
    "| 1   | Move chicken up        |\n",
    "| 2   | Move chicken down      |\n",
    "\n",
    "\n",
    "### Observation Space\n",
    "\n",
    "The observation is a `ndarray` with shape `(3,84,84)` an image of the state as a human player would see it.\n",
    "\n",
    "Install this repo as a dependency:\n",
    "\n",
    "```\n",
    "import sys\n",
    "!{sys.executable} -m pip install -e ./reafference\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a89b2334",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/anaconda3/envs/reaff/lib/python3.8/site-packages/gym/envs/registration.py:423: UserWarning: \u001b[33mWARN: Custom namespace `ALE` is being overridden by namespace `ALE`. If you are developing a plugin you shouldn't specify a namespace in `register` calls. The namespace is specified through the entry point package metadata.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ben/anaconda3/envs/reaff/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "/home/ben/anaconda3/envs/reaff/lib/python3.8/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import gym\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import reafference.jnu as J\n",
    "\n",
    "from reafference import environment\n",
    "from reafference.environment.freeway import ground_truth, make_dataset, make_episode, make\n",
    "from reafference.model import UNet, DiagLinear\n",
    "\n",
    "\n",
    "DEVICE = \"cuda:0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83c1c4d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3, 84, 84) (199, 3, 84, 84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/anaconda3/envs/reaff/lib/python3.8/site-packages/ipywidgets/widgets/widget.py:477: DeprecationWarning: Passing unrecognized arguments to super(Canvas).__init__(scale=1).\n",
      "object.__init__() takes exactly one argument (the instance to initialize)\n",
      "This is deprecated in traitlets 4.2.This error will be raised in a future release of traitlets.\n",
      "  super(Widget, self).__init__(**kwargs)\n",
      "/home/ben/anaconda3/envs/reaff/lib/python3.8/site-packages/ipykernel/pylab/backend_inline.py:10: DeprecationWarning: `ipykernel.pylab.backend_inline` is deprecated, directly use `matplotlib_inline.backend_inline`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1575807249ba45119f7143ba760cd8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='x', layout=Layout(width='99%'), max=198), Output()), _do…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6c057f471b44f28b596f689a031d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Canvas(height=264, width=1017),), layout=Layout(align_items='center', display='flex', flex_flow…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Exploration\n",
    "env = make()\n",
    "state, action, info = make_episode(env, max_length=200)\n",
    "ram = info['ram']\n",
    "gt_effect, re_effect, ex_effect = ground_truth(env, state, ram)\n",
    "\n",
    "b = -np.ones_like(gt_effect[...,:1]) # padding\n",
    "imgs = (np.concatenate([gt_effect, b, re_effect, b, ex_effect], axis=3) + 1) / 2\n",
    "imgs = np.concatenate([state[:-1], b, imgs], axis=3)\n",
    "\n",
    "# action to image\n",
    "aindx = np.eye(env.action_space.n).repeat(imgs.shape[-1]/3, 1)#.repeat(6, 0)[np.newaxis,...].repeat(3,0)\n",
    "aindx = aindx[action[:-1]]\n",
    "aindx = aindx[:,np.newaxis, np.newaxis,:].repeat(3,1).repeat(4,2)\n",
    "\n",
    "imgs = np.concatenate([aindx, imgs], axis=-2)\n",
    "J.images(imgs, on_interact=action, scale=3)\n",
    "imgs = np.concatenate([aindx, imgs], axis=-2)\n",
    "#torchvision.io.write_video(\"./images/freeway_ground_truth.mp4\", torch.from_numpy(imgs[...,:-1].transpose(0,2,3,1) * 255).int(), fps=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b3872e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET SIZE: 4995\n"
     ]
    }
   ],
   "source": [
    "# make a dataset with a random policy\n",
    "env = make()\n",
    "dataset = make_dataset(env, num_episodes=5, max_episode_length=1000)\n",
    "print(f\"DATASET SIZE: {len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9269d1be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNet to estimate effects\n",
    "state_shape = env.observation_space.shape\n",
    "action_shape = (env.action_space.n,)\n",
    "latent_shape = (512,)\n",
    "epochs = 50\n",
    "\n",
    "model = UNet(state_shape[0], state_shape[0], exp=5, output_activation=torch.nn.Tanh(), batch_normalize=False)\n",
    "conditional_shape = model.conditional_shape(state_shape)\n",
    "model.condition = DiagLinear(conditional_shape, action_shape=action_shape[0])\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "import torchinfo\n",
    "torchinfo.summary(model, input_data=(torch.zeros(2, *state_shape), torch.zeros(2, *action_shape)))\n",
    "\n",
    "model.load_state_dict(torch.load(\"./models/Freeway-v0.model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a89eef1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9357383c018c4ef6a6a60e3d61df7351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x1, x2, a \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m---> 14\u001b[0m     x1, x2, a \u001b[38;5;241m=\u001b[39m \u001b[43mx1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m, x2\u001b[38;5;241m.\u001b[39mto(DEVICE), a\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     15\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# prediction of the total effect of each action\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=False)\n",
    "    \n",
    "# Train\n",
    "epoch_iter = tqdm(range(epochs))\n",
    "\n",
    "for e in epoch_iter:\n",
    "    \n",
    "    # make a dataset with a random policy\n",
    "\n",
    "    #dataset = make_dataset(env, num_episodes=100, device=DEVICE)\n",
    "    #\n",
    "    avg_loss = []\n",
    "    for x1, x2, a in loader:\n",
    "        x1, x2, a = x1.to(DEVICE), x2.to(DEVICE), a.to(DEVICE)\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # prediction of the total effect of each action\n",
    "        pred_total_effect = model(x1, a)        \n",
    "        # prediction of the exafferent effect - when all actions are noop  \n",
    "        noop = torch.zeros_like(a)\n",
    "        noop[:,0] = 1. \n",
    "        pred_exafferent_effect = model(x1, noop)\n",
    "        \n",
    "        # prediction of the reafferent effect (total - exafferent)\n",
    "        # this will be 0 for any a == 0 in the batch\n",
    "        pred_reafferent_effect = pred_total_effect - pred_exafferent_effect.detach()\n",
    "        #pred_reafferent_effect[a[:,0] == 1] = 0. # detach gradients where reafferent effect should be 0 (?)\n",
    "    \n",
    "        pred_effect = pred_exafferent_effect + pred_reafferent_effect # combined effect\n",
    "        total_effect = x2 - x1 # ground truth total effect\n",
    "        \n",
    "        loss = criterion(pred_effect, total_effect)\n",
    "        loss.backward()\n",
    "        avg_loss.append(loss.detach())\n",
    "        optim.step()\n",
    "    \n",
    "    avg_loss = torch.stack(avg_loss).cpu().numpy().mean()\n",
    "    epoch_iter.set_description(f\"Loss: {avg_loss : .5f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fca6593",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3, 84, 84) (199, 3, 84, 84)\n",
      "torch.Size([199, 3, 3, 425]) torch.Size([199, 3, 84, 425])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42794117e387476f912421c3ef09aa33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='x', layout=Layout(width='99%'), max=198), Output()), _do…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc762d606a44058923352fb56b930c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Canvas(height=261, width=1275),), layout=Layout(align_items='center', display='flex', flex_flow…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision.transforms.functional as fn\n",
    "\n",
    "def get_images(state, action, total_effect, t_effect, re_effect, ex_effect):\n",
    "    b = -torch.ones_like(total_effect[...,:1]).cpu()\n",
    "    imgs = (torch.cat([b, total_effect.cpu(), b, t_effect.cpu(), b, re_effect.cpu(), b, ex_effect.cpu(), b], dim=-1) + 1) / 2\n",
    "    imgs = torch.cat([state[:-1].cpu(), imgs], dim=-1)\n",
    "    ai = fn.resize(torch.eye(3)[action[:-1]].unsqueeze(1).unsqueeze(1).repeat(1,3,1,1), size=(3,imgs.shape[-1]), interpolation=0)\n",
    "    \n",
    "    print(ai.shape, imgs.shape)\n",
    "    imgs = torch.cat([imgs, ai], dim=-2)\n",
    "    return imgs\n",
    "\n",
    "env = make()\n",
    "state, action, info = make_episode(env, max_length=200)\n",
    "ram = info['ram']\n",
    "gt_effect, re_effect, ex_effect = ground_truth(env, state, ram)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x1, a = torch.from_numpy(state[:-1]).to(DEVICE), torch.eye(3)[torch.from_numpy(action[:-1])].to(DEVICE)\n",
    "    x1, a = x1.contiguous(), a.contiguous()\n",
    "    pred_total = model(x1, a)\n",
    "    noop = torch.zeros_like(a)\n",
    "    noop[:,0] = 1. \n",
    "    pred_ex = model(x1, noop)\n",
    "    pred_re = pred_total - pred_ex\n",
    "\n",
    "gt_effect = torch.from_numpy((state[1:] - state[:-1]))\n",
    "imgs = get_images(torch.from_numpy(state), torch.from_numpy(action), gt_effect, pred_total, pred_re, pred_ex)\n",
    "    \n",
    "J.images(imgs, scale=3, on_interact=action)\n",
    "\n",
    "imgs = fn.resize(imgs, size=[2*imgs.shape[2], 2*imgs.shape[3]], interpolation=torchvision.transforms.functional.InterpolationMode.NEAREST)\n",
    "\n",
    "video = (imgs.permute(0,2,3,1) * 255).int()[:200]\n",
    "torchvision.io.write_video(f\"./media/Freeway-1-Predictions.mp4\", video, fps=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec00c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"./models/Freeway-v0.model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ccef68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b88c52b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9968e9ef992e48ff87c8cfccffe941d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Canvas(height=630, width=480),), layout=Layout(align_items='center', display='flex', flex_flow=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<reafference.jnu.image._image.Image at 0x7ff468937190>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# figure for paper...\n",
    "x = env.unwrapped.reset()\n",
    "for i in range(40):\n",
    "    x, *_ = env.unwrapped.step(0)\n",
    "\n",
    "n = 2\n",
    "z = x[111:195,15:99,:].copy()\n",
    "x[111-n:195+n,15-n:99+n,:] = np.array([255,0,0])\n",
    "\n",
    "x[111:195,15:99,:] = z\n",
    "J.image(x, scale=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e9c6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reaff",
   "language": "python",
   "name": "reaff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
